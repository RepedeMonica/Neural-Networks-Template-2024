{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMygj5mdI96S9WP/4p3Eygd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RepedeMonica/Neural-Networks-Template-2024/blob/main/NN4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading the data"
      ],
      "metadata": {
        "id": "PYxJ_jSBYfZp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fI3fOhB2YEQ9",
        "outputId": "28b35b28-ad84-46ef-f74b-e3e058d4c65a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 725kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.77MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "image_to_tensor = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = MNIST('data', train=True, transform=image_to_tensor, download=True)\n",
        "test_dataset = MNIST('data', train=False, transform=image_to_tensor, download=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASETS"
      ],
      "metadata": {
        "id": "n-OeTA7s9sON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DATASETS\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.data = [dataset[i][0] for i in range(len(dataset))]\n",
        "        self.labels = [dataset[i][1] for i in range(len(dataset))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        data = self.data[i].view(-1)\n",
        "        label = self.labels[i]\n",
        "        return data, label\n",
        "dataset = MNISTDataset(train_dataset)\n",
        "val_dataset = MNISTDataset(test_dataset)"
      ],
      "metadata": {
        "id": "5sRKbFMlYdFK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader"
      ],
      "metadata": {
        "id": "trnRFXfgcnp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DATALOADERS\n",
        "from torch.utils.data import DataLoader\n",
        "dataset_loader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=False)\n"
      ],
      "metadata": {
        "id": "40Wj1u6vcqf9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MY MODEL"
      ],
      "metadata": {
        "id": "1hOEqM4vk5-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "class MyNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(MyNN,self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.layer_2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.layer_3 = nn.Linear(hidden_size2, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        # x is a Tensor of size [batch size, input_size]\n",
        "        x = self.layer_1(x) # x is a Tensor of size [batch size, hidden_size]\n",
        "        x = self.relu(x)  # We can use any activation function\n",
        "\n",
        "        x = self.layer_2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer_3(x)\n",
        "        return x\n",
        "model = MyNN(input_size=784, hidden_size1=128, hidden_size2=64, output_size=10)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHhos4gbk7pQ",
        "outputId": "77f93b79-0cc2-4ebf-8c0e-8ff0909f9686"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyNN(\n",
            "  (layer_1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (layer_2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (layer_3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEVICES"
      ],
      "metadata": {
        "id": "pbtfqDdAnRvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DEVICES\n",
        "import torch\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')  # On multi-gpu workstation we can select cuda:0, cuda:1, ...\n",
        "    if torch.mps.is_available():\n",
        "        return torch.device('mps')\n",
        "    return torch.device('cpu')\n",
        "device = get_device()\n",
        "print(device)\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Rj9hfu4nS8y",
        "outputId": "519edd67-3c63-4cda-8719-786fecdb483d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPTIMIZERS"
      ],
      "metadata": {
        "id": "ID1qwgLJn-8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#OPTIMIZATOR\n",
        "# Optimizers apply the gradients calculated by the Autograd engine to the weights, using their own optimization technique\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True,weight_decay=0.001)  # SGD with Nesterov momentum and weight decay\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.001)  # Adam with Weight Decay\n",
        "# Schedulers change the learning rate enabling faster training\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10,gamma=0.5)  # Learning rate scheduler, halves the learning rate each 10 steps\n"
      ],
      "metadata": {
        "id": "fdF1kpcHoAXc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOSS *FUNCTION*"
      ],
      "metadata": {
        "id": "QHmx8yi4oJRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LOSS\n",
        "criterion = nn.CrossEntropyLoss()  # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n"
      ],
      "metadata": {
        "id": "xXYce6j9oKtm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN"
      ],
      "metadata": {
        "id": "FKaZYclnoOPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN\n",
        "import csv\n",
        "def train(model, train_dataloader, criterion, optimizer, device):\n",
        "    model.train()  # We need to activate the dropout & batch norm layers\n",
        "\n",
        "    mean_loss = 0.0\n",
        "\n",
        "    for data, labels in train_dataloader:\n",
        "        data = data.to(device)  # We move the data to device. Bonus: we can do this in an async manner using non_blocking and pin_memory\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(data)  # the forward pass\n",
        "        loss = criterion(outputs, labels)  # we calculate the loss\n",
        "\n",
        "        optimizer.zero_grad() # reset the gradients\n",
        "        loss.backward()  # we backpropagate the loss\n",
        "\n",
        "        optimizer.step()  # we update the weights\n",
        "\n",
        "        mean_loss += loss.item()\n",
        "\n",
        "    mean_loss /= len(train_dataloader)\n",
        "    return mean_loss\n"
      ],
      "metadata": {
        "id": "rSplGsuGoPE-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "validation"
      ],
      "metadata": {
        "id": "0bs7bg3AoqbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()  # it is better to decorate the method with torch.inference_mode or torch.no_grad\n",
        "def val(model, val_dataloader, criterion, device):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    mean_loss = 0.0\n",
        "    model.eval()  # We need to deactivate the dropout & batch norm layers\n",
        "\n",
        "    for data, labels in val_dataloader:\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        loss = criterion(output, labels) # we calculate the loss\n",
        "        _, predicted = output.max(1)\n",
        "        num_correct += (predicted == labels).sum()\n",
        "        num_samples += predicted.size(0)  # size of the mini batch\n",
        "\n",
        "        mean_loss += loss.item()\n",
        "\n",
        "    mean_loss /= len(val_dataloader)\n",
        "    return mean_loss, num_correct / num_samples\n"
      ],
      "metadata": {
        "id": "mCcyRsjporx5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATE PREDICTIONS TO CSV"
      ],
      "metadata": {
        "id": "rAv7gcs1-Ge1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def generate_predictions_csv(model, val_dataloader, device, output_csv=\"submisions.csv\"):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for i, (data, labels) in enumerate(val_dataloader):\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(data)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.size(0)  #\n",
        "\n",
        "        for idx, pred in enumerate(predicted):\n",
        "            predictions.append((i * val_dataloader.batch_size + idx, pred.item()))\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions * 100\n",
        "    print(f\"Final test accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    with open(output_csv, mode=\"w\", newline=\"\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"ID\", \"target\"])\n",
        "        writer.writerows(predictions)\n",
        "\n",
        "    print(f\"Predictions were saved in {output_csv}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-h4VAoXD-KeQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "traininf loop"
      ],
      "metadata": {
        "id": "wjm01PCgo2_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN LOOP\n",
        "from tqdm import tqdm\n",
        "def main(model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs):\n",
        "    with tqdm(tuple(range(epochs))) as tbar:\n",
        "        for epoch in tbar:\n",
        "            train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
        "            val_loss, accuracy = val(model, val_dataloader, criterion, device)\n",
        "            scheduler.step()\n",
        "            tbar.set_description(f\"Train loss: {train_loss:.5f} | Val loss: {val_loss:.5f} | Val accuracy: {accuracy:.5f}\")\n",
        "        generate_predictions_csv(model, val_dataloader, device)\n",
        "\n",
        "\n",
        "main(model, dataset_loader, val_dataloader, criterion, optimizer, device, 150)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aWBd9GJo4fL",
        "outputId": "a0e0735b-0d47-4cfb-a72b-31f3654e9b06"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.03086 | Val loss: 0.06210 | Val accuracy: 0.98230: 100%|██████████| 150/150 [04:45<00:00,  1.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test accuracy: 98.23%\n",
            "Predictions were saved in submisions.csv\n"
          ]
        }
      ]
    }
  ]
}